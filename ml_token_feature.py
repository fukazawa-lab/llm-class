# -*- coding: utf-8 -*-
"""ML_token_feature_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vvLyUXIRFHQ0bICGtmZqmk0c1CAY5fVt

# トークンを特徴量にした機械学習による文書分類
"""

!pip install transformers[ja,torch] datasets matplotlib japanize-matplotlib

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier  # Random Forestのクラス
from transformers import AutoTokenizer
from typing import Tuple
import pandas as pd
from datasets import load_dataset, Dataset, ClassLabel
from sklearn.metrics import precision_score, recall_score


# データセットの読み込み
# train_dataset = load_dataset("llm-book/wrime-sentiment", split="train")
# valid_dataset = load_dataset("llm-book/wrime-sentiment", split="validation")
# train_dataset = load_dataset("shunk031/JGLUE", name="MARC-ja",split="train")
# valid_dataset = load_dataset("shunk031/JGLUE", name="MARC-ja",split="validation")
# CSVファイルからデータを読み込む
train_df = pd.read_csv('train.csv')
valid_df = pd.read_csv('validation.csv')
train_dataset = Dataset.from_pandas(train_df)
valid_dataset = Dataset.from_pandas(valid_df)

# データセットを結合
all_sentences = train_dataset['sentence'] + valid_dataset['sentence']
all_labels = train_dataset['label'] + valid_dataset['label']

# トークナイズと特徴量化
tokenizer = AutoTokenizer.from_pretrained("cl-tohoku/bert-base-japanese-v3")
tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in all_sentences]
tokenized_sentences = [' '.join(tokens) for tokens in tokenized_sentences]

# 特徴量の作成
vectorizer = TfidfVectorizer(max_features=10000, stop_words="english")
X = vectorizer.fit_transform(tokenized_sentences)

# トレーニングデータとバリデーションデータの分割
num_train_samples = len(train_dataset)
X_train = X[:num_train_samples]
X_valid = X[num_train_samples:]
train_labels = all_labels[:num_train_samples]
valid_labels = all_labels[num_train_samples:]

# Random Forestモデルの訓練
clf = RandomForestClassifier()
clf.fit(X_train, train_labels)

# バリデーションデータで予測
valid_predictions = clf.predict_proba(X_valid)

# 正解率の計算

def compute_metrics(eval_pred: Tuple[np.ndarray, np.ndarray]) -> dict[str, float]:
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    # Calculate Precision
    precision = precision_score(labels, predictions, average='macro')  # または average='micro' など適切なオプションを選択してください

    # Calculate Recall
    recall = recall_score(labels, predictions, average='macro')  # または average='micro' など適切なオプションを選択してください

    # Calculate Accuracy
    accuracy = (predictions == labels).mean()

    return {"accuracy": accuracy, "precision": precision, "recall": recall}

metrics_dict = compute_metrics((valid_predictions, valid_labels))
accuracy = metrics_dict["accuracy"]
precision = metrics_dict["precision"]
recall = metrics_dict["recall"]

print("Validation Accuracy:", accuracy)
print("Validation Precision:", precision)
print("Validation Recall:", recall)

"""### （参考） XGBoost"""

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from xgboost import XGBClassifier
from transformers import AutoTokenizer
from datasets import load_dataset
from typing import Tuple
import pandas as pd
from datasets import Dataset, DatasetDict

def compute_accuracy(
    eval_pred: Tuple[np.ndarray, np.ndarray]
) -> dict[str, float]:
    """予測ラベルと正解ラベルから正解率を計算"""
    predictions, labels = eval_pred
    # predictionsは各ラベルについてのスコア
    # 最もスコアの高いインデックスを予測ラベルとする
    predictions = np.argmax(predictions, axis=1)
    return {"accuracy": (predictions == labels).mean()}

# データセットの読み込み
# train_dataset = load_dataset("llm-book/wrime-sentiment", split="train")
# valid_dataset = load_dataset("llm-book/wrime-sentiment", split="validation")
# train_dataset = load_dataset("shunk031/JGLUE", name="MARC-ja",split="train")
# valid_dataset = load_dataset("shunk031/JGLUE", name="MARC-ja",split="validation")

# # ローカルファイルから読み込み
# CSVファイルからデータを読み込む
train_df = pd.read_csv('train.csv')
valid_df = pd.read_csv('validation.csv')
train_dataset = Dataset.from_pandas(train_df)
valid_dataset = Dataset.from_pandas(valid_df)

# train_dataset = load_dataset('csv', data_files='train.csv', header=0)
# valid_dataset =load_dataset('csv', data_files='validation.csv',header=0)



# データセットを結合
all_sentences = train_dataset['sentence'] + valid_dataset['sentence']
all_labels = train_dataset['label'] + valid_dataset['label']

# Tokenizerのロード
tokenizer = AutoTokenizer.from_pretrained("cl-tohoku/bert-base-japanese-v3")

# トークナイズと特徴量化
tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in all_sentences]
tokenized_sentences = [' '.join(tokens) for tokens in tokenized_sentences]

# 特徴量の作成
vectorizer = TfidfVectorizer(max_features=10000, stop_words="english")
X = vectorizer.fit_transform(tokenized_sentences)

# トレーニングデータとバリデーションデータの分割
num_train_samples = len(train_dataset)
X_train = X[:num_train_samples]
X_valid = X[num_train_samples:]
train_labels = all_labels[:num_train_samples]
valid_labels = all_labels[num_train_samples:]

# XGBoostモデルの訓練
clf = XGBClassifier()
clf.fit(X_train, train_labels)

# バリデーションデータで予測
valid_predictions = clf.predict_proba(X_valid)

# 正解率の計算
accuracy_dict = compute_accuracy((valid_predictions, valid_labels))
accuracy = accuracy_dict["accuracy"]
print("Validation Accuracy:", accuracy)

"""###  （参考）  LightGBM"""

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from lightgbm import LGBMClassifier  # LightGBMのクラス
from transformers import AutoTokenizer
from datasets import load_dataset
from typing import Tuple

def compute_accuracy(
    eval_pred: Tuple[np.ndarray, np.ndarray]
) -> dict[str, float]:
    """予測ラベルと正解ラベルから正解率を計算"""
    predictions, labels = eval_pred
    # predictionsは各ラベルについてのスコア
    # 最もスコアの高いインデックスを予測ラベルとする
    predictions = np.argmax(predictions, axis=1)
    return {"accuracy": (predictions == labels).mean()}

# データセットの読み込み
# train_dataset = load_dataset("llm-book/wrime-sentiment", split="train")
# valid_dataset = load_dataset("llm-book/wrime-sentiment", split="validation")
# train_dataset = load_dataset("shunk031/JGLUE", name="MARC-ja",split="train")
# valid_dataset = load_dataset("shunk031/JGLUE", name="MARC-ja",split="validation")

# データを読み込む
# CSVファイルからデータを読み込む
train_df = pd.read_csv('train.csv')
valid_df = pd.read_csv('validation.csv')
train_dataset = Dataset.from_pandas(train_df)
valid_dataset = Dataset.from_pandas(valid_df)


# データセットを結合
all_sentences = train_dataset['sentence'] + valid_dataset['sentence']
all_labels = train_dataset['label'] + valid_dataset['label']

# Tokenizerのロード
tokenizer = AutoTokenizer.from_pretrained("cl-tohoku/bert-base-japanese-v3")

# トークナイズと特徴量化
tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in all_sentences]
tokenized_sentences = [' '.join(tokens) for tokens in tokenized_sentences]

# 特徴量の作成
vectorizer = TfidfVectorizer(max_features=10000, stop_words="english")
X = vectorizer.fit_transform(tokenized_sentences)

# トレーニングデータとバリデーションデータの分割
num_train_samples = len(train_dataset)
X_train = X[:num_train_samples]
X_valid = X[num_train_samples:]
train_labels = all_labels[:num_train_samples]
valid_labels = all_labels[num_train_samples:]

# LightGBMモデルの訓練
clf = LGBMClassifier()
clf.fit(X_train, train_labels)

# バリデーションデータで予測
valid_predictions = clf.predict_proba(X_valid)

# 正解率の計算
accuracy_dict = compute_accuracy((valid_predictions, valid_labels))
accuracy = accuracy_dict["accuracy"]
print("Validation Accuracy:", accuracy)